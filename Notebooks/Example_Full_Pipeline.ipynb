{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this notebook, we demonstrate our pipeline on speeches related to taxation from the US Congress records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '/cluster/work/lawecon/Projects/Ash_Gauthier_Widmer/germain/narrative-nlp'\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../Code/\")\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from utils import (\n",
    "    tokenize_into_sentences,\n",
    "    filter_sentences,\n",
    "    preprocess,\n",
    "    UsedRoles,\n",
    "    Document,\n",
    "    dict_concatenate,\n",
    "    get_verb_counts,\n",
    "    clean_verbs\n",
    ")\n",
    "\n",
    "from word_embedding import run_word2vec, compute_embedding, USE, SIF_Word2Vec\n",
    "from semantic_role_labeling import SRL, extract_roles, postprocess_roles, extract_role_per_sentence\n",
    "from clustering import Clustering, label_clusters, label_clusters_most_freq\n",
    "from sklearn.cluster import KMeans\n",
    "from cooccurrence import build_df, CoOccurrence\n",
    "\n",
    "used_roles = UsedRoles()\n",
    "used_roles[\"ARG2\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Run SRL\n",
    "\n",
    "Make sure you correctly downloaded the pre-trained SRL model from AllenNLP. \n",
    "\n",
    "This step takes a bit of time. You may want make yourself some coffee and run it only once. If you have multiple cores on your machine, you may also change n_cores to speed up the process.\n",
    "\n",
    "We save results in \"../Output/SRL/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob('../Data/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.predictors.predictor import Predictor\n",
    "predictor = Predictor.from_path(\"../srl-model-2018.05.25.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(filepath, batch_size = 20, max_chars = 500):\n",
    "    print(filepath)\n",
    "    outfilename = '../Output/SRL' + re.sub(\"../Data\", '', str(filepath)) + '.json'\n",
    "    outfilename = re.sub('.txt', '', outfilename)\n",
    "    with open(outfilename, 'w') as json_file:\n",
    "        res = []\n",
    "        with open(filepath, \"r\") as f:\n",
    "            speech = f.read()\n",
    "            sentences = tokenize_into_sentences(speech)\n",
    "            batches = [sentences[x:x+batch_size] for x in range(0, len(sentences), batch_size)]\n",
    "            for batch in batches:\n",
    "                batch = [{'sentence': str(S)} for S in batch if len(S) < max_chars]\n",
    "                temp = predictor.predict_batch_json(batch)\n",
    "                res = res + temp\n",
    "            json.dump(res, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "\n",
    "print('Starting SRL job...')\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "Parallel(n_jobs=n_cores)(delayed(process_text)(filepath)\n",
    "                            for filepath in filepaths)\n",
    "\n",
    "t1 = time.time() - t0\n",
    "print('SRL job took %s seconds.' %t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train Word Embeddings\n",
    "\n",
    "We save the model in '../Output/Embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepaths = glob.glob('../Data/*.txt')\n",
    "\n",
    "print('Processing speeches to train embeddings...')\n",
    "\n",
    "time.sleep(1)\n",
    "\n",
    "with open('../Output/Text/clean_speeches.txt', 'w') as f1:\n",
    "    for filepath in tqdm(filepaths):\n",
    "        with open(filepath, \"r\") as f2:\n",
    "            speech = f2.read()\n",
    "            sentences = tokenize_into_sentences(speech)\n",
    "            sentences = preprocess(sentences)\n",
    "            for sentence in sentences:\n",
    "                f1.write(sentence + '\\n')\n",
    "\n",
    "print('Processing Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First option: take existing word embeddings and train the model some more\n",
    "# We have a small corpus in this example, so we start with pretrained embeddings.\n",
    "\n",
    "import gensim\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "\n",
    "sentences = LineSentence('../Output/Text/clean_speeches.txt')\n",
    "\n",
    "pretrained_model = KeyedVectors.load_word2vec_format(\"../glove_2_word2vec.6B.300d.txt\", binary = False)\n",
    "\n",
    "model = Word2Vec(size = 300, window = 8, min_count = 1, workers = 1)\n",
    "model.build_vocab(sentences)\n",
    "total_examples = model.corpus_count\n",
    "model.build_vocab([list(pretrained_model.vocab.keys())], update = True)\n",
    "model.intersect_word2vec_format(\"../glove_2_word2vec.6B.300d.txt\", binary = False, lockf = 1.0)\n",
    "model.train(sentences, total_examples = total_examples, epochs = model.epochs)\n",
    "model.save('../Output/Embeddings/my_word2vec_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Second option: train your own word embeddings (if your corpus is large enough)\n",
    "\n",
    "# import gensim\n",
    "# from gensim.models import Word2Vec\n",
    "# from gensim.models.word2vec import LineSentence\n",
    "\n",
    "# t0 = time.time()\n",
    "\n",
    "# print('Training word embeddings...')\n",
    "\n",
    "# sentences = LineSentence('../Output/Text/clean_speeches.txt', max_sentence_length=30000)\n",
    "\n",
    "# model = Word2Vec(sentences, size = 300, window = 8, min_count = 1, workers = 1)\n",
    "\n",
    "# model.save('../Output/Embeddings/my_word2vec_model.model')\n",
    "\n",
    "# t1 = time.time() - t0\n",
    "\n",
    "# print('Train the Word2Vec model took %s seconds.' %t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Process SRL output\n",
    "\n",
    "Default processing of the text data is minimal by default. We remove lowercase, remove punctuation and digits. Many additional options are available (see documentation).\n",
    "\n",
    "We save results in \"../Output/Processed_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sif_w2v = SIF_Word2Vec(\"../Output/Embeddings/my_word2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(\"../Output/SRL/*.json\")\n",
    "\n",
    "documents_all = []\n",
    "postproc_roles_all = []\n",
    "sentence_index_all = []  # np.array([], dtype=np.uint32)\n",
    "vectors_all = []  # None\n",
    "statement_index_all = []  # {}\n",
    "funny_index_all = []  # {}\n",
    "\n",
    "\n",
    "def do_all(filenames):\n",
    "    global documents_all, postproc_roles_all, sentence_index_all, vectors_all, statement_index_all, funny_index_all\n",
    "\n",
    "    start_index = 0\n",
    "\n",
    "    for filename in tqdm(filenames):\n",
    "        with open(filename) as json_file:\n",
    "            srl_res = json.load(json_file)\n",
    "        \n",
    "        roles, sentence_index = extract_roles(srl_res, start=start_index)\n",
    "\n",
    "        postproc_roles = postprocess_roles(roles, stop_words = my_stopwords)\n",
    "\n",
    "        sif_vectors, sif_statements_index, sif_funny_index = compute_embedding(\n",
    "            sif_w2v, statements=postproc_roles, used_roles=used_roles, start=start_index\n",
    "        )\n",
    "\n",
    "        documents_all.append(Document(filename, start_index))\n",
    "        postproc_roles_all.extend(postproc_roles)\n",
    "        sentence_index_all.append(sentence_index)\n",
    "        vectors_all.append(sif_vectors)\n",
    "        statement_index_all.append(sif_statements_index)\n",
    "        funny_index_all.append(sif_funny_index)\n",
    "\n",
    "        start_index += sentence_index.size\n",
    "\n",
    "do_all(filenames)\n",
    "sentence_index_all = np.concatenate(sentence_index_all)\n",
    "vectors_all = dict_concatenate(vectors_all)\n",
    "statement_index_all = dict_concatenate(statement_index_all)\n",
    "funny_index_all = dict_concatenate(funny_index_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb_counts = get_verb_counts(postproc_roles_all)\n",
    "postproc_roles_all = clean_verbs(postproc_roles_all, verb_counts = verb_counts)\n",
    "postproc_roles_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are %s documents.' %len(documents_all))\n",
    "print('There are %s statements.' %len(postproc_roles_all))\n",
    "\n",
    "argO = [' '.join(i['ARGO']) for i in postproc_roles_all if 'ARGO' in i.keys()]\n",
    "argO = set(argO)\n",
    "print('There are %s unique agents.' %len(argO))\n",
    "\n",
    "arg1 = [' '.join(i['ARG1']) for i in postproc_roles_all if 'ARG1' in i.keys()]\n",
    "arg1 = set(arg1)\n",
    "print('There are %s unique patients.' %len(arg1))\n",
    "\n",
    "BV = [' '.join(i['B-V']) for i in postproc_roles_all if 'B-V' in i.keys()]\n",
    "BV = set(BV)\n",
    "print('There are %s unique verbs.' %len(BV))\n",
    "\n",
    "arg2 = [' '.join(i['ARG2']) for i in postproc_roles_all if 'ARG2' in i.keys()]\n",
    "arg2 = set(arg2)\n",
    "print('There are %s unique attributes.' %len(arg2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Output/Processed_files/postproc_roles_all.json\", 'w') as f:\n",
    "    json.dump(postproc_roles_all, f)\n",
    "\n",
    "with open(\"../Output/Processed_files/documents_all.json\", 'w') as f:\n",
    "    json.dump(documents_all, f)\n",
    "    \n",
    "np.savez('../Output/Processed_files/sentence_index_all.npz', sentence_index_all)      \n",
    "np.savez('../Output/Processed_files/statement_index_all.npz', **statement_index_all)\n",
    "np.savez('../Output/Processed_files/vectors_all.npz', **vectors_all)\n",
    "np.savez('../Output/Processed_files/funny_index_all.npz', **funny_index_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Cluster Roles\n",
    "\n",
    "We save results in \"../Output/Clustering/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Output/Processed_files/documents_all.json\") as f:\n",
    "    documents_all_raw = json.load(f)\n",
    "\n",
    "with open(\"../Output/Processed_files/postproc_roles_all.json\") as f:\n",
    "    postproc_roles_all = json.load(f)\n",
    "    \n",
    "sentence_index_all = np.load('../Output/Processed_files/sentence_index_all.npz')['arr_0']\n",
    "\n",
    "statement_index_all = np.load('../Output/Processed_files/statement_index_all.npz')\n",
    "statement_index_all = {fi: statement_index_all[fi] for fi in statement_index_all.files}\n",
    "\n",
    "funny_index_all = np.load('../Output/Processed_files/funny_index_all.npz')\n",
    "funny_index_all = {fi: funny_index_all[fi] for fi in funny_index_all.files}\n",
    "\n",
    "vectors_all = np.load('../Output/Processed_files/vectors_all.npz')\n",
    "vectors_all = {fi: vectors_all[fi] for fi in vectors_all.files}\n",
    "\n",
    "# Format them in the narratives-nlp Document format\n",
    "documents_all = []\n",
    "for i in documents_all_raw:\n",
    "    documents_all.append(Document(i[0], i[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The threshold hyperparameter allows the user to control for cluster coherence. \n",
    "# If the distance between the centroid and the vector role is above the threshold, then this observation is labeled as noise. \n",
    "# Threshold=2 assumes no noise and forcefully assigns a label to each semantic role in the data.\n",
    "\n",
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of clusters for each role. Here, we divide the total number of unique tokens for each role by 100.\n",
    "\n",
    "bins = 100\n",
    "\n",
    "num_argO_clu = round(len(argO)/bins)\n",
    "num_arg1_clu = round(len(arg1)/bins)\n",
    "num_arg2_clu = round(len(arg2)/bins)\n",
    "\n",
    "print(num_argO_clu, num_arg1_clu, num_arg2_clu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans=KMeans(random_state=0, n_init = 1)\n",
    "\n",
    "clustering = Clustering(\n",
    "    cluster=kmeans,\n",
    "    n_clusters={\"ARGO\": num_argO_clu, \"ARG1\": num_arg1_clu, \"ARG2\": num_arg2_clu},\n",
    "    used_roles=used_roles,\n",
    ")\n",
    "\n",
    "sample_vectors = clustering.resample(vectors=vectors_all, sample_size=1)\n",
    "clustering_fit = clustering.fit(vectors=sample_vectors)\n",
    "clustering_res = clustering.predict(vectors=vectors_all)\n",
    "distance = clustering.compute_distance(vectors=vectors_all, predicted_cluster = clustering_res)\n",
    "clustering_mask = clustering.distance_mask(distance, threshold=threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clustering, open(\"../Output/Clustering/clustering.pickle\", 'wb'))\n",
    "np.savez('../Output/Clustering/clustering_res.npz', **clustering_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Label Clusters and Explore Resulting Narratives\n",
    "\n",
    "We save results in \"../Output/Narratives\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../Output/Clustering/clustering.pickle\", 'rb') as f:\n",
    "    clustering = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_clusters_most_freq(\n",
    "    clustering_res=clustering_res,\n",
    "    postproc_roles=postproc_roles_all,\n",
    "    statement_index=statement_index_all,\n",
    "    clustering_mask=clustering_mask\n",
    ")\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = build_df(\n",
    "    clustering_res=clustering_res,\n",
    "    postproc_roles=postproc_roles_all,\n",
    "    statement_index=statement_index_all,\n",
    "    used_roles=used_roles,\n",
    "    clustering_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_bis = labels.copy()\n",
    "df_bis = df.copy()\n",
    "\n",
    "labels_bis['ARGO'][pd.NA] = [(pd.NA, 0)]\n",
    "labels_bis['ARG1'][pd.NA] = [(pd.NA, 0)]\n",
    "labels_bis['ARG2'][pd.NA] = [(pd.NA, 0)]\n",
    "\n",
    "def display_label(x, labels_bis, arg):\n",
    "    if x in labels_bis[arg]:\n",
    "        res = labels_bis[arg][x][0][0]\n",
    "    else:\n",
    "        res = pd.NA\n",
    "    return res\n",
    "\n",
    "df_bis['ARGO'] = df_bis['ARGO'].apply(lambda x: display_label(x, labels_bis, 'ARGO'))\n",
    "df_bis['ARG1'] = df_bis['ARG1'].apply(lambda x: display_label(x, labels_bis, 'ARG1'))\n",
    "df_bis['ARG2'] = df_bis['ARG2'].apply(lambda x: display_label(x, labels_bis, 'ARG2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis = df_bis.replace({np.NaN: ''})\n",
    "df_bis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis[(df_bis.ARGO != '') & (df_bis.ARG1 != '') & (df_bis['B-V'] != '')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bis.to_csv('../Output/Narratives/df_with_labels.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
